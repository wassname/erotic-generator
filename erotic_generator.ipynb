{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perth Machine Learning Group Poem Generator\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The following code uses GRU to generate poems. It reads through a corpus of poems, and learns sequences of characters, including line breaks and titles.\n",
    "\n",
    "In short, it observes many sequence of characters, and infers the character that should come next. For instance, it guesses that after 'The cat eat' should come the letter 's'.\n",
    "\n",
    "Further details will be given with the code.\n",
    "\n",
    "## The code\n",
    "\n",
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T02:53:53.215830Z",
     "start_time": "2018-11-03T02:53:51.102040Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  # version 1.9 or above\n",
    "tf.enable_eager_execution()  # Execution of code as it runs in the notebook. Normally, TensorFlow looks up the whole code before execution for efficiency.\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import unidecode\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T02:53:53.222780Z",
     "start_time": "2018-11-03T02:53:53.219041Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_file = 'data/erotic_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T02:53:54.835754Z",
     "start_time": "2018-11-03T02:53:53.226109Z"
    }
   },
   "outputs": [],
   "source": [
    "text = unidecode.unidecode(open(path_to_file).read())\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T02:53:54.944800Z",
     "start_time": "2018-11-03T02:53:54.839132Z"
    }
   },
   "outputs": [],
   "source": [
    "unique = sorted(set(text))  # unique contains all the unique characters in the corpus\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(unique)}  # maps characters to indexes\n",
    "idx2char = {i:u for i, u in enumerate(unique)}  # maps indexes to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T02:53:54.959689Z",
     "start_time": "2018-11-03T02:53:54.947850Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 100  # Maximum length sentence we want per input in the network\n",
    "vocab_size = len(unique)\n",
    "embedding_dim = 256  # number of 'meaningful' features to learn. Ex: ['queen', 'king', 'man', 'woman'] has a least 2 embedding dimension: royalty and gender.\n",
    "units = 1024  # In keras: number of output of a sequence. In short it rem\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T02:53:55.924464Z",
     "start_time": "2018-11-03T02:53:54.962057Z"
    }
   },
   "outputs": [],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for f in range(0, len(text) - max_length, max_length):\n",
    "    inps = text[f : f + max_length]\n",
    "    targ = text[f + 1 : f + 1 + max_length]\n",
    "    input_text.append([char2idx[i] for i in inps])\n",
    "    target_text.append([char2idx[t] for t in targ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T02:54:10.336892Z",
     "start_time": "2018-11-03T02:53:55.927781Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination\n",
    "\n",
    "In fact, the algorithm does not learn which characters comes next. It analyzes sequences of characters as inputs (ex: 'abcd'), and predicts sequences as outputs (ex: 'bcde').\n",
    "\n",
    "Why?\n",
    "\n",
    "During the training phase, it learns more that just the next character. It updates weights for each characters from the input sequence to the output sequence.\n",
    "\n",
    "> Consider the sequences 'abcd', 'bcde', 'cdef', 'defg', the letter \"d\" is given different weights that depend on the previous sequences\n",
    "\n",
    "The use of these updates helps predicting better the next sequences and so on. So it learns the next character but also all the weights \n",
    "\n",
    "The next chunk of code is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T02:54:10.345334Z",
     "start_time": "2018-11-03T02:54:10.340346Z"
    }
   },
   "outputs": [],
   "source": [
    "# example of input:\n",
    "print('Given the following sequence: \\n\\n')\n",
    "print(''.join(idx2char[input_text[14][i]] for i in range(len(target_text[0]))))\n",
    "print('\\n\\n')\n",
    "print('the network has to learn that a correct continuation is: \\n')\n",
    "# example of output the algorithm has to learn\n",
    "print(''.join(idx2char[target_text[14][i]] for i in range(len(input_text[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T02:54:10.361108Z",
     "start_time": "2018-11-03T02:54:10.347737Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "    super(Model, self).__init__()\n",
    "    self.units = units\n",
    "    self.batch_sz = batch_size\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    if tf.test.is_gpu_available():\n",
    "      self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "      self.gru = tf.keras.layers.GRU(self.units, \n",
    "                                     return_sequences=True, \n",
    "                                     return_state=True, \n",
    "                                     recurrent_activation='sigmoid', \n",
    "                                     recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, states = self.gru(x, initial_state=hidden)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x = self.fc(output)\n",
    "#     x = tf.log_sigmoid(x)\n",
    "    return x, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T02:54:10.382931Z",
     "start_time": "2018-11-03T02:54:10.363461Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(vocab_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T02:54:10.395298Z",
     "start_time": "2018-11-03T02:54:10.385863Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T02:54:10.408158Z",
     "start_time": "2018-11-03T02:54:10.398360Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(real, preds):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T03:15:13.118684Z",
     "start_time": "2018-11-03T02:54:10.411161Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "    hidden = model.reset_states()  # initializes the hidden state at the start of every epoch\n",
    "    \n",
    "    for (batch, (inp, target)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "              predictions, hidden = model(inp, hidden)  # feeds the hidden state back into the model\n",
    "              target = tf.reshape(target, (-1, ))  # reshapes for the loss function\n",
    "              loss = loss_function(target, predictions)\n",
    "              \n",
    "          grads = tape.gradient(loss, model.variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "          if batch % 100 == 0:\n",
    "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, loss))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T04:24:19.315888Z",
     "start_time": "2018-11-03T04:24:19.310745Z"
    }
   },
   "outputs": [],
   "source": [
    "m=1.2\n",
    "lighten=lambda x:1-(1/m-x/m)\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-03T04:41:12.873Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_strings = [\n",
    "    'magnificent', \n",
    "    'bodice', \n",
    "    'secret',\n",
    "    'nuns',\n",
    "    'belong',\n",
    "    'by God',\n",
    "    'all overcome', \n",
    "    'his eyes', \n",
    "    'why',\n",
    "    'yielding',\n",
    "    'flutter',\n",
    "    'wives',\n",
    "    'swooned', \n",
    "    'Mrs',\n",
    "    'delicious',\n",
    "    'loins',\n",
    "    'body',\n",
    "    'greatest',\n",
    "    'alone',\n",
    "    'merely',\n",
    "    'Spontaneously',\n",
    "    'frightful',\n",
    "    'intimate',\n",
    "    'clinging',\n",
    "    'neglect',\n",
    "    'gushed',\n",
    "    'grace',\n",
    "    'drunk',\n",
    "    'panting',\n",
    "    'honey',\n",
    "    \n",
    "]\n",
    "\n",
    "html_generateds = []\n",
    "for start_string in start_strings:\n",
    "    num_generate = 200  # number of characters to generate\n",
    "#     start_string = 'The lady'  # beginning of the generated text. TODO: try start_string = ' '\n",
    "\n",
    "    input_eval = [char2idx[s] for s in start_string]  # converts start_string to numbers the model understands\n",
    "    input_eval = tf.expand_dims(input_eval, 0)  # \n",
    "\n",
    "    text_generated = ''\n",
    "    html_generated = ''\n",
    "    text_generated += start_string\n",
    "    html_generated += '<b>{}</b>'.format(start_string)\n",
    "\n",
    "    temperature = 0.9  # arger is more creative\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    for i in range(num_generate):\n",
    "        predictions, hidden = model(input_eval, hidden)  # predictions holds the probability for each character to be most adequate continuation\n",
    "\n",
    "        predictions = predictions / temperature  # alters characters' probabilities to be picked (but keeps the order)\n",
    "        predicted_id = tf.multinomial(tf.exp(predictions), num_samples=1)[0][0].numpy()  # picks the next character for the generated text\n",
    "        predicted_probs = tf.nn.softmax(logits=predictions)[0, predicted_id].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated += idx2char[predicted_id]  # appends\n",
    "        html_generated+='<span style=\"color:rgba(0,0,0,{a:1.1f})\">{c:}</span>'.format(\n",
    "            c=idx2char[predicted_id],\n",
    "            a=lighten(predicted_probs))\n",
    "\n",
    "    text_generated += '\\n\\n'\n",
    "    html_generated += '<p/><hr/>'\n",
    "    \n",
    "    display(HTML(html_generated))\n",
    "    html_generateds.append(html_generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T04:34:25.417191Z",
     "start_time": "2018-11-03T04:34:25.395049Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T04:34:30.000478Z",
     "start_time": "2018-11-03T04:34:29.989494Z"
    }
   },
   "outputs": [],
   "source": [
    "html=\"\"\"\n",
    "    <html><body>\n",
    "    {}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\".format('<hr/>'.join(html_generateds))\n",
    "\n",
    "display(HTML(html))\n",
    "with open('outputs/erotics_predictions.html', 'w') as fo:\n",
    "    fo.write(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "228px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "526px",
    "left": "0px",
    "right": "1008px",
    "top": "149px",
    "width": "368px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
